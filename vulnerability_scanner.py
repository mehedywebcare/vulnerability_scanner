import requests
from urllib.parse import urlparse

def check_status_code(url):
  """
  Checks the response status code of a URL.
  """
  try:
    response = requests.get(url)
    response.raise_for_status()  # Raise exception for non-2xx codes
    print(f"URL: {url} - Status Code: {response.status_code} (Success)")
  except requests.exceptions.RequestException as e:
    print(f"URL: {url} - Error: {e}")

def check_robots_txt(url):
  """
  Checks for the presence and content of robots.txt.
  """
  parsed_url = urlparse(url)
  robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"  
  try:
    response = requests.get(robots_txt)
    if response.status_code == 200:
      print(f"URL: {url} - robots.txt Found")
      # Consider parsing robots.txt content for potential issues (optional)
  except requests.exceptions.RequestException:
    pass  # Ignore errors for robots.txt

def check_common_vulnerable_directories(url, base_url):
  """
  Checks for existence of common vulnerable directories (basic check).
  """
  directories = [
      "/admin/", "/wp-admin/", "/phpmyadmin/", "/phpinfo.php"  # Added phpinfo.php
  ]
  for directory in directories:
    vulnerable_url = f"{base_url}{directory}"
    try:
      response = requests.get(vulnerable_url)
      if response.status_code != 404:  # Not a 404 Not Found
        print(f"URL: {vulnerable_url} - Potentially Vulnerable Directory Found")
    except requests.exceptions.RequestException:
      pass  # Ignore errors for individual directory checks

def main():
  """
  Main function for user input and testing.
  """
  url = input("Enter a website URL: ")
  parsed_url = urlparse(url)
  base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"

  check_status_code(url)
  check_robots_txt(url)
  check_common_vulnerable_directories(url, base_url)

  print("\nDisclaimer: This is a basic scanner and doesn't guarantee finding vulnerabilities. Always get permission before testing any website.")

if __name__ == "__main__":
  main()
